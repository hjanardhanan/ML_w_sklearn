{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of chip data; prepare in separate pipelines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "# Data load\n",
    "filename = 'chip_dataset.csv'\n",
    "file = os.path.abspath(os.path.join(os.getcwd(),'..', filename))\n",
    "df = pd.read_csv(file)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'binary'\n",
    "target_label = 'Type' if mode == 'binary' else 'Foundry'\n",
    "df.drop(['Unnamed: 0', 'Product', 'Release Date'], axis = 1, inplace=True)\n",
    "df.drop('Foundry' if mode == 'binary' else 'Type', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util to get NaNs\n",
    "def get_null_pc(df1, out = False) :\n",
    "    gt_threshold = 0.58\n",
    "    null_map = {'gt':{}, 'lt':{}, 'none' : {}, 'other':{}}\n",
    "    null_pc_map = 1 - df1.count()/len(df1)\n",
    "    if out == True :\n",
    "        print(null_pc_map)\n",
    "    for key, val in null_pc_map.items() :\n",
    "        pc_type = 'other'\n",
    "        if val == 0.0 :\n",
    "            pc_type = 'none'\n",
    "        elif val > gt_threshold :\n",
    "            pc_type = 'gt'\n",
    "        elif val < 0.05 and val != 0 :\n",
    "            pc_type = 'lt'\n",
    "\n",
    "        if key not in null_map[pc_type] :\n",
    "            null_map[pc_type][key] = [val]\n",
    "        else :\n",
    "            null_map[pc_type][key].append(val)\n",
    "        \n",
    "    return null_map\n",
    "# print(df.info())\n",
    "null_pc = get_null_pc(df, out = False)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First round of null drops\n",
    "# Drop rows with < 5% is NaNs\n",
    "# print (\"Dropping rows of \", list(null_pc['lt'].keys()))\n",
    "df.dropna(subset = null_pc['lt'].keys(), inplace=True)\n",
    "\n",
    "# Drop entire col if col > 85% NaNs\n",
    "# print (\"Dropping cols : \", null_pc['gt'].keys())\n",
    "if 'FP16 GFLOPS' in df.columns :\n",
    "    df.drop(null_pc['gt'].keys(), axis = 1, inplace = True)\n",
    "# print (df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X + Y\n",
    "X = df.loc[:, list(set(df.columns) - set([target_label]))]\n",
    "Y = df.loc[:, [target_label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# Transform target label\n",
    "le = LabelEncoder()\n",
    "Y[target_label] =  le.fit_transform(Y[target_label])\n",
    "\n",
    "# One hot encode rest of categorical data \n",
    "# #TBD: Study ordinal vs binary effect on scores\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "obj_vals = list(set(X.columns) - set(X._get_numeric_data().columns))\n",
    "enc.fit(X.loc[:,obj_vals])\n",
    "# print(enc.categories_)\n",
    "enc_df = pd.DataFrame(enc.transform(X.loc[:, obj_vals]))\n",
    "# print(X.shape, \"\\n\", enc_df.shape)\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "enc_df.reset_index(drop=True, inplace=True)\n",
    "X.drop(obj_vals, axis=1, inplace=True)\n",
    "X = pd.concat([X,enc_df], axis=1, ignore_index=False)\n",
    "# print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in NA values\n",
    "from sklearn.impute import KNNImputer\n",
    "cols_w_nans = list(get_null_pc(X)['other'])\n",
    "# print (cols_w_nans)\n",
    "knn_imputer = KNNImputer(n_neighbors = 1)\n",
    "X[cols_w_nans] = knn_imputer.fit_transform(X[cols_w_nans])\n",
    "# print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transistors (million)\n",
      "1178.0    4.660832\n",
      "1400.0    4.660832\n",
      "106.0     2.407002\n",
      "4800.0    2.363239\n",
      "1300.0    1.991247\n",
      "            ...   \n",
      "8.0       0.021882\n",
      "463.0     0.021882\n",
      "49.0      0.021882\n",
      "420.0     0.021882\n",
      "198.0     0.021882\n",
      "Name: count, Length: 178, dtype: float64\n",
      "Freq (MHz)\n",
      "2000.0    2.975930\n",
      "2400.0    2.954048\n",
      "300.0     2.800875\n",
      "500.0     2.800875\n",
      "2200.0    2.691466\n",
      "            ...   \n",
      "515.0     0.021882\n",
      "795.0     0.021882\n",
      "1177.0    0.021882\n",
      "1176.0    0.021882\n",
      "416.0     0.021882\n",
      "Name: count, Length: 473, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Outlier detection : up sampling ? down sampling ? Drop ?\n",
    "import re\n",
    "non_categ_cols = []\n",
    "for col in list(X.columns) :\n",
    "    if re.search(\"^[a-zA-Z]\", str(col)) != None :\n",
    "        non_categ_cols.append(col)\n",
    "# Drop outliers which make < outlier_threshold\n",
    "outlier_threshold = 6\n",
    "for col in non_categ_cols :\n",
    "    Q3, Q1 = X[col].quantile(0.75), X[col].quantile(0.25)\n",
    "    IQR = Q3 - Q1\n",
    "    threshold = 1.5\n",
    "    outlier = X[(X[col] < Q1 - threshold * IQR) | (X[col] > Q3 + threshold * IQR)]\n",
    "    pc_outlier = len(outlier) *100 / len(X)\n",
    "    if pc_outlier < outlier_threshold and pc_outlier > 0 :\n",
    "        X = X.drop(outlier.index)\n",
    "    # else :\n",
    "        # print (X[col].value_counts() / X[col].count() * 100)\n",
    "        # No need for up/down sampling since max is 5% share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data : Standardize/Normalize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
