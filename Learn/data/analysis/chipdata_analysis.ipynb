{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of chip data; prepare in separate pipelines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "# Data load\n",
    "filename = 'chip_dataset.csv'\n",
    "file = os.path.abspath(os.path.join(os.getcwd(),'..', filename))\n",
    "df = pd.read_csv(file)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = ['Type',  'Foundry']\n",
    "df.drop(['Unnamed: 0', 'Product', 'Release Date'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util to get NaNs\n",
    "def get_null_pc(df1, out = False) :\n",
    "    gt_threshold = 0.58\n",
    "    null_map = {'gt':{}, 'lt':{}, 'none' : {}, 'other':{}}\n",
    "    null_pc_map = 1 - df1.count()/len(df1)\n",
    "    if out == True :\n",
    "        print(null_pc_map)\n",
    "    for key, val in null_pc_map.items() :\n",
    "        pc_type = 'other'\n",
    "        if val == 0.0 :\n",
    "            pc_type = 'none'\n",
    "        elif val > gt_threshold :\n",
    "            pc_type = 'gt'\n",
    "        elif val < 0.05 and val != 0 :\n",
    "            pc_type = 'lt'\n",
    "\n",
    "        if key not in null_map[pc_type] :\n",
    "            null_map[pc_type][key] = [val]\n",
    "        else :\n",
    "            null_map[pc_type][key].append(val)\n",
    "        \n",
    "    return null_map\n",
    "# print(df.info())\n",
    "null_pc = get_null_pc(df, out = False)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First round of null drops\n",
    "# Drop rows with < 5% is NaNs\n",
    "# print (\"Dropping rows of \", list(null_pc['lt'].keys()))\n",
    "df.dropna(subset = null_pc['lt'].keys(), inplace=True)\n",
    "\n",
    "# Drop entire col if col > 85% NaNs\n",
    "# print (\"Dropping cols : \", null_pc['gt'].keys())\n",
    "if len(null_pc['gt'].keys()) > 0 :\n",
    "    df.drop(null_pc['gt'].keys(), axis = 1, inplace = True)\n",
    "# print (df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X + Y\n",
    "X = df.loc[:, list(set(df.columns) - set(target_label))]\n",
    "Y = df.loc[:, target_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# Transform target label\n",
    "les = []\n",
    "for label in target_label :\n",
    "    le = LabelEncoder()\n",
    "    Y[label] =  le.fit_transform(Y[label])\n",
    "    les.append(le)\n",
    "\n",
    "# One hot encode rest of categorical data \n",
    "# #TBD: Study ordinal vs binary effect on scores\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "obj_vals = list(set(X.columns) - set(X._get_numeric_data().columns))\n",
    "enc.fit(X.loc[:,obj_vals])\n",
    "# print(enc.categories_)\n",
    "enc_df = pd.DataFrame(enc.transform(X.loc[:, obj_vals]))\n",
    "# print(X.shape, \"\\n\", enc_df.shape)\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "enc_df.reset_index(drop=True, inplace=True)\n",
    "X.drop(obj_vals, axis=1, inplace=True)\n",
    "X = pd.concat([X,enc_df], axis=1, ignore_index=False)\n",
    "# print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in NA values\n",
    "from sklearn.impute import KNNImputer\n",
    "cols_w_nans = list(get_null_pc(X)['other'])\n",
    "# print (cols_w_nans)\n",
    "knn_imputer = KNNImputer(n_neighbors = 1)\n",
    "X[cols_w_nans] = knn_imputer.fit_transform(X[cols_w_nans])\n",
    "# print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection : up sampling ? down sampling ? Drop ?\n",
    "import re\n",
    "non_categ_cols = []\n",
    "for col in list(X.columns) :\n",
    "    if re.search(\"^[a-zA-Z]\", str(col)) != None :\n",
    "        non_categ_cols.append(col)\n",
    "# Drop outliers which make < outlier_threshold\n",
    "outlier_threshold = 6\n",
    "for col in non_categ_cols :\n",
    "    Q3, Q1 = X[col].quantile(0.75), X[col].quantile(0.25)\n",
    "    IQR = Q3 - Q1\n",
    "    threshold = 1.5\n",
    "    outlier = X[(X[col] < Q1 - threshold * IQR) | (X[col] > Q3 + threshold * IQR)]\n",
    "    pc_outlier = len(outlier) *100 / len(X)\n",
    "    if pc_outlier < outlier_threshold and pc_outlier > 0 :\n",
    "        X = X.drop(outlier.index)\n",
    "    # else :\n",
    "        # print (X[col].value_counts() / X[col].count() * 100)\n",
    "        # No need for up/down sampling since max is 5% share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data : Standardize/Normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X[non_categ_cols] = StandardScaler().fit_transform(X[non_categ_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out data : chip_dataset_cleaned.csv\n",
    "df_out = pd.concat([X, Y], axis=1)\n",
    "df_out.to_csv('../chip_dataset_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out encodings to file\n",
    "dir = \"../\"\n",
    "for i, label in enumerate(target_label) :\n",
    "    filename = dir + str(label) + \".csv\"\n",
    "    vendor_out = open(filename, 'w')\n",
    "    vendor_out.writelines(list(les[i].classes_))\n",
    "    vendor_out.close()\n",
    "\n",
    "foundry_out = open('../Vendor.csv', 'w')\n",
    "for f in enc.categories_ :\n",
    "    foundry_out.write(str(f))\n",
    "foundry_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
